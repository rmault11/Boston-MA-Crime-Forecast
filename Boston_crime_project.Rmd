---
title: "Pstat 174 Boston Crime Project"
author: "Riley Mault"
date: "February 18, 2020"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)


# Loading necessary packages
library(dplyr)
library(tidyr)
library(lubridate)
library(moments)
library(ggplot2)
library(ggfortify)
library(tseries)
library(forecast)
library(qpcR) 
```

## 1. Abstract

Crime incident reports are provided by Boston Police Department (BPD) to document the initial details surrounding an incident to which BPD officers respond. This is a dataset containing records capturing the type of incident as well as when and where it occurred. For this time series project, I will be focusing exclusively on when crimes occurred. Records begin in June 14, 2015 and continue to September 3, 2018.

I will be addressing questions such as: Does the frequency of crimes have any pattern?  If the frequency of crimes does have a pattern, why is that? Is it possibly to forecast the daily frequency of crimes?

To address these questions, I plotted the data ordered by time, analyzed auto-correlation plots, and developed seasonal ARIMA models to predict/forecast future values.

## 2. 

## Introduction

My goal of this daily Boston crime report dataset is to find and observe frequencies at which crimes occur. In observing those frequencies, I will build an ARIMA model to forecast the frequencies of future crime reports.

## Importing and Cleaning Data

```{r load_dataset}
# import dataset
crime_df = read.csv('crime.csv')
attach(crime_df)
```

```{r sort_data}
# sorting data by date of crime
crime_df = crime_df  %>% separate(OCCURRED_ON_DATE, c("Date", "Time"), sep = " ") %>% mutate(Date = ymd(Date))
crime_df$Date = as.Date(crime_df$Date)
crime_df = crime_df[order(crime_df$Date),]
```

## Distribution of the Counts of Crimes by Date

```{r hist}
ts_data_full = ts(table(crime_df$Date))

# split up data into train/test split
ts_data = ts(ts_data_full[c(1:1067)])
ts_data.test = ts(ts_data_full[c(1068:1077)])

# histogram of our crime data
hist(ts_data, 50, col='light blue', main='Crimes Count Distribution')
paste(c('Skewness is', skewness(ts_data)), collapse = ' ')
```

I separated the data into 50 bins and plotted a histogram. Most days, about 250 - 300 crimes occur. We can see that the data seems pretty normal with a very slight left skew. Therefore, a transformation isn't necessary.

## Distribution of Crimes by Time

```{r timeSeries, fig.width = 14}

# Ploting timeseries plot
ts.plot(ts_data, main = "Crimes by Time")
abline(h=mean(ts_data), col='red')


# Transformation of data but not needed/performed

# bcTransform<-boxcox(ts_data ~ as.numeric(1:length(ts_data)))
# bcTransform$x[which(bcTransform$y== max(bcTransform$y))]
# lambda = bcTransform$x[which(bcTransform$y== max(bcTransform$y))]
# lambda = bcTransform$x[which(bcTransform$y== max(bcTransform$y))]
# ts.bc = (1/lambda)*(ts_data^lambda-1)
# hist(ts.bc, 50, col='light blue', main='Crimes Count Distribution')
# shapiro.test(ts.bc)
```

From the chart above, there is noticeable seasonality that we will address in order to make our data stationary. We can see a repeating pattern (almost that of a sin wave) every 365 days.

## ACF and PACF

```{r acf, fig.height=5, fig.width=12}
par(mfrow=c(1,2))

# ACF and PACF
acf(table(crime_df$Date), 40, main='Autocorrelation Lag=40')
pacf(table(crime_df$Date), 40, main='Partial Autocorrelation Lag=40')
```

In both the ACF and PACF plots, there are lag spikes every 7 lags. To approach this, I will difference the data by 7 lag units. The partial correlation shows many significant lags. We can conclude that crimes are correlated with yesterday and the same day in each previous week.

```{r seasonality, fig.width=14}
# Decompose ts to observe trend/seasonality
y = ts(as.ts(ts_data), frequency=7)
plot(decompose(y))
```

In decomposing the crime data, we notice a small amount of trend and noticeable seasonality. Again, this gives us more reason to difference the data to make it stationary.

## Differencing Data

```{r differencing, fig.width=14}
# Differencing the model by 7 to remove seasonality/trend
ts_7 = diff(ts_data, lag=7)

plot.ts(ts_7, main="Crimes by Time Differenced at lag 7")
abline(h=mean(ts_7), col='red')
```

## Dickey-Fuller Test for Stationary

```{r stationary_testing}
adf.test(ts_7)
```

After differencing the data by 7, the time series graph looks much more stationary. The p-value of the Dickey-Fuller Test (smaller than .01) being less than .05 confirms the alternative hypothesis that the data is stationary.

## Differenced ACF and PACF

```{r acf_ts_1, fig.height=5, fig.width=12}
par(mfrow=c(1,2))

acf(ts_7, 50, main='Autocorrelation of ts_7')
pacf(ts_7,50, main='Partial Autocorrelation ts_7')
```

After differencing our data, the ACF and PACF plots look much cleaner and easier to analyze. In the ACF plot the first few lags show significance. From this, we can possibly assign a q value of 2 or 3 for the non-seasonal component. There is another significant spike at 7 due to the seasonal component; so we assign Q=1.

In the PACF plot, there is one significant lag at lag 1. However, we might be better off having a complete MA model and having no AR components to the non-seasonal aspect. There are also continuous decreasing lags every 7 lags. From this, can attribute a seasonal AR component with order of at least 3 or 4.

## ARIMA Models

```{r models}
# Model A
fit.A = arima(ts_data, order=c(0,1,3), seasonal = list(order = c(4,1,1), period = 7),  method="CSS")
fit.A
AICc(arima(ts_data, order=c(0,1,3), seasonal = list(order = c(4,1,1), period = 7),  method="ML"))

# Model B
fit.B = arima(ts_data, order=c(0,1,2), seasonal = list(order = c(4,1,1), period = 7), method="CSS")
fit.B
AICc(arima(ts_data, order=c(0,1,2), seasonal = list(order = c(4,1,1), period = 7), method="ML"))
```

After testing out many similar models to the ps and qs suggested from the ACF/PACF plots, these two models (Model A and Model B) resulted in the lowest AICc values and best fitting models. Model A has MA order 3 coefficients while Model B has MA order 2. Model B has a slightly lower AICc value by 2. To decide which model is better, we will run diagnostics on both.

## Diagnostic Checking for Model A

$$\Delta_7$$$$U_t$$ = $(1-0.7326B-0.1575B^{2}-0.0236B^{3})(1-0.9644B^{7})$$Z_t$ + $(0.065B^{7}-0.0066B^{14}-0.1432B^{21}-0.0179B^{28})$$X_t$

```{r DiagnosticsModelA, fig.width=6, fig.height=4}

# Checking Invertibility of model A
autoplot(fit.A)

# residual hist
res = residuals(fit.A)
hist(res,density=20,breaks=20, col="blue", xlab="", prob=TRUE)
m = mean(res)
std = sqrt(var(res))
curve( dnorm(x,m,std), add=TRUE )
plot.ts(res)
fitt = lm(res ~ as.numeric(1:length(res))); abline(fitt, col="red")
abline(h=mean(res), col="blue")

# Normal Q-Q to check for normality of res
qqnorm(res,main= "Normal Q-Q Plot for Model A")
qqline(res,col="blue")

acf(res, lag.max=40)
pacf(res, lag.max=40)

# Tests to check fit
df = 3
shapiro.test(res)
Box.test(res, lag = 7, type = c("Box-Pierce"), fitdf= df)
Box.test(res, lag = 7, type = c("Ljung-Box"), fitdf= df)
Box.test(res^2, lag = 7, type = c("Ljung-Box"), fitdf= 0)
```


## Diagnostic Checking for Model B

$$\Delta_7$$$$U_t$$ = $(1-0.7347B-0.1771B^{2})(1-0.9642B^{7})$$Z_t$ + $(-0.0677B^{7}-0.0090B^{14}-0.1462B^{21}-0.0183B^{28})$$X_t$

```{r DiagnosticsModelB, fig.width=6, fig.height=4}

#Checking Invertibility of model B
autoplot(fit.B)

# residual hist
res = residuals(fit.B)
hist(res,density=20,breaks=20, col="blue", xlab="", prob=TRUE)
m = mean(res)
std = sqrt(var(res))
curve( dnorm(x,m,std), add=TRUE )
plot.ts(res)
fitt = lm(res ~ as.numeric(1:length(res))); abline(fitt, col="red")
abline(h=mean(res), col="blue")

# Normal Q-Q to check for normality of res
qqnorm(res,main= "Normal Q-Q Plot for Model B")
qqline(res,col="blue")

acf(res, lag.max=40)
pacf(res, lag.max=40)

# Tests to check fit
df = 2
shapiro.test(res)
Box.test(res, lag = 7, type = c("Box-Pierce"), fitdf= df)
Box.test(res, lag = 7, type = c("Ljung-Box"), fitdf= df)
Box.test(res^2, lag = 7, type = c("Ljung-Box"), fitdf= 0)
```

Our diagnostics for both models both pretty solid. Both models pass all the necessary tests of fit (since the p-values are all greater than 0.05), except the Shapiro-Wilk normality test. Both models fail the normality test. However, when looking at the histogram and normal Q-Q plot of the residuals, we see that they all look pretty good and normal. Even after attempting to make a model using box-cox transformed data, it still fails the residual normality test. So even though we have this test error issue, the histogram and normal Q-Q plots give us solid evidence that our model is normally distributed. All the roots for both models are within the unit circle. And lastly, the ACF/PACF plots of the residuals look great. Only 2 at max protrude out of the 95% interval boundary. 

## Forecasting Model A

```{r ForecastingA, fig.height=4}

# Predicting future values
forecast(fit.A)
pred.A = predict(fit.A, n.ahead= 110)
U.A= pred.A$pred+ 1.96*pred.A$se
L.A= pred.A$pred-1.96*pred.A$se
ts.plot(ts_data, xlim=c(950,length(ts_data)+110))
lines(U.A, col="blue", lty=2)
lines(L.A, col="blue", lty=2)
lines(1068:1177, pred.A$pred, col="red")

# Adding original data to predictions
ts.plot(ts_data_full, xlim= c(950,length(ts_data)+110), col="black")
lines(U.A, col="blue", lty="dashed")
lines(L.A, col="blue", lty="dashed")
lines((length(ts_data)+1):(length(ts_data)+110), pred.A$pred, col="red")
lines((length(ts_data)+1):(length(ts_data)+110), pred.A$pred, col="red")
```

## Forecasting Model B

```{r ForecastingB, fig.height=4}

# Predicting future values
forecast(fit.B)
pred.B = predict(fit.B, n.ahead= 110)
U.B= pred.B$pred+ 1.96*pred.B$se
L.B= pred.B$pred-1.96*pred.B$se
ts.plot(ts_data, xlim=c(950,length(ts_data)+110))
lines(U.B, col="blue", lty=2)
lines(L.B, col="blue", lty=2)
lines(1068:1177, pred.B$pred, col="red")

# Adding original data to predictions
ts.plot(ts_data_full, xlim= c(950,length(ts_data)+110), col="black")
lines(U.B, col="blue", lty="dashed")
lines(L.B, col="blue", lty="dashed")
lines((length(ts_data)+1):(length(ts_data)+110), pred.B$pred, col="red")
lines((length(ts_data)+1):(length(ts_data)+110), pred.B$pred, col="red")
```

After examining the forecast predictions for both models, we can see that they are extremely similar. They both show somewhat accurate predictions and do a good job articulating the weekly seasonality. As it attempts to predict more, it very slightly begins to over/under predict possibly due to the changing yearly seasonality (Not uncommon to see in models such as these). Because it is very difficult to notice differences, we will choose Model B being better based on its lower AICc value.

## 3. Conclusion

After analyzing the time series and acf plots of the crime data, it is apparent that there is a pattern in the frequency of crimes. It seems that there is a yearly pattern as well as a weekly pattern. Friday appears to have he highest number of crimes per week, while Sunday the least. From the data, it is not quite clear why most crimes occur on Friday. Maybe we can attribute it to Friday night being the most popular night of the week to leave the house; And Sunday the least likely night of the week to leave the house. However, there are many parameters that can attribute to this, and we need more outside data to identify the reason.

Due to the strong seasonality of the data, it was necessary to use a SARIMA model in order to forecast future data. From understanding the weekly seasonal pattern, I was able to make a couple SARIMA models that accurately forecast the daily frequency of crimes. Although the residuals of both models appeared normal in the diagnostic plots, it is important to note that they did not pass the Shapiro-Wilk normality test.








## 4. References

Dataset
https://www.kaggle.com/AnalyzeBoston/crimes-in-boston

Skewness
https://www.r-bloggers.com/measures-of-skewness-and-kurtosis/

Decompose
http://r-statistics.co/Time-Series-Analysis-With-R.html

Adf Stationary Testing
https://nwfsc-timeseries.github.io/atsa-labs/sec-boxjenkins-aug-dickey-fuller.html

Forecast
https://robjhyndman.com/hyndsight/forecast7-ggplot2/











## 5. Appendix

```{r Appendix, eval=FALSE}
# import dataset
crime_df = read.csv('crime.csv')
attach(crime_df)

# sorting data by date of crime
crime_df = crime_df  %>% separate(OCCURRED_ON_DATE, c("Date", "Time"), sep = " ") %>% mutate(Date = ymd(Date))
crime_df$Date = as.Date(crime_df$Date)
crime_df = crime_df[order(crime_df$Date),]

ts_data_full = ts(table(crime_df$Date))

# split up data into train/test split
ts_data = ts(ts_data_full[c(1:1067)])
ts_data.test = ts(ts_data_full[c(1068:1077)])

# histogram of our crime data
hist(ts_data, 50, col='light blue', main='Crimes Count Distribution')
paste(c('Skewness is', skewness(ts_data)), collapse = ' ')

# Ploting timeseries plot
ts.plot(ts_data, main = "Crimes by Time")
abline(h=mean(ts_data), col='red')


# Transformation of data but not needed/performed

# bcTransform<-boxcox(ts_data ~ as.numeric(1:length(ts_data)))
# bcTransform$x[which(bcTransform$y== max(bcTransform$y))]
# lambda = bcTransform$x[which(bcTransform$y== max(bcTransform$y))]
# lambda = bcTransform$x[which(bcTransform$y== max(bcTransform$y))]
# ts.bc = (1/lambda)*(ts_data^lambda-1)
# hist(ts.bc, 50, col='light blue', main='Crimes Count Distribution')
# shapiro.test(ts.bc)

# ACF and PACF
acf(table(crime_df$Date), 40, main='Autocorrelation Lag=40')
pacf(table(crime_df$Date), 40, main='Partial Autocorrelation Lag=40')


# Decomposing to examine seasonality/trend
y = ts(as.ts(ts_data), frequency=7)
plot(decompose(y))

# Differencing the model by 7 to remove seasonality/trend
ts_7 = diff(ts_data, lag=7)

plot.ts(ts_7, main="Crimes by Time Differenced at lag 7")
abline(h=mean(ts_7), col='red')

adf.test(ts_7)

acf(ts_7, 50, main='Autocorrelation of ts_7')
pacf(ts_7,50, main='Partial Autocorrelation ts_7')

# Model A
fit.A = arima(ts_data, order=c(0,1,3), seasonal = list(order = c(4,1,1), period = 7),  method="CSS")
fit.A
AICc(arima(ts_data, order=c(0,1,3), seasonal = list(order = c(4,1,1), period = 7),  method="ML"))

# Model B
fit.B = arima(ts_data, order=c(0,1,2), seasonal = list(order = c(4,1,1), period = 7), method="CSS")
fit.B
AICc(arima(ts_data, order=c(0,1,2), seasonal = list(order = c(4,1,1), period = 7), method="ML"))

# Checking Invertibility of model A
autoplot(fit.A)

# residual hist
res = residuals(fit.A)
hist(res,density=20,breaks=20, col="blue", xlab="", prob=TRUE)
m = mean(res)
std = sqrt(var(res))
curve( dnorm(x,m,std), add=TRUE )
plot.ts(res)
fitt = lm(res ~ as.numeric(1:length(res))); abline(fitt, col="red")
abline(h=mean(res), col="blue")

# Normal Q-Q to check for normality of res
qqnorm(res,main= "Normal Q-Q Plot for Model A")
qqline(res,col="blue")

acf(res, lag.max=40)
pacf(res, lag.max=40)

# Tests to check fit
df = 3
shapiro.test(res)
Box.test(res, lag = 7, type = c("Box-Pierce"), fitdf= df)
Box.test(res, lag = 7, type = c("Ljung-Box"), fitdf= df)
Box.test(res^2, lag = 7, type = c("Ljung-Box"), fitdf= 0)

#Checking Invertibility of model B
autoplot(fit.B)

# residual hist
res = residuals(fit.B)
hist(res,density=20,breaks=20, col="blue", xlab="", prob=TRUE)
m = mean(res)
std = sqrt(var(res))
curve( dnorm(x,m,std), add=TRUE )
plot.ts(res)
fitt = lm(res ~ as.numeric(1:length(res))); abline(fitt, col="red")
abline(h=mean(res), col="blue")

# Normal Q-Q to check for normality of res
qqnorm(res,main= "Normal Q-Q Plot for Model B")
qqline(res,col="blue")

acf(res, lag.max=40)
pacf(res, lag.max=40)

# Tests to check fit
df = 2
shapiro.test(res)
Box.test(res, lag = 7, type = c("Box-Pierce"), fitdf= df)
Box.test(res, lag = 7, type = c("Ljung-Box"), fitdf= df)
Box.test(res^2, lag = 7, type = c("Ljung-Box"), fitdf= 0)

# Predicting future values
forecast(fit.A)
pred.A = predict(fit.A, n.ahead= 110)
U.A= pred.A$pred+ 1.96*pred.A$se
L.A= pred.A$pred-1.96*pred.A$se
ts.plot(ts_data, xlim=c(950,length(ts_data)+110))
lines(U.A, col="blue", lty=2)
lines(L.A, col="blue", lty=2)
lines(1068:1177, pred.A$pred, col="red")

# Adding original data to predictions
ts.plot(ts_data_full, xlim= c(950,length(ts_data)+110), col="black")
lines(U.A, col="blue", lty="dashed")
lines(L.A, col="blue", lty="dashed")
lines((length(ts_data)+1):(length(ts_data)+110), pred.A$pred, col="red")
lines((length(ts_data)+1):(length(ts_data)+110), pred.A$pred, col="red")

# Predicting future values
forecast(fit.B)
pred.B = predict(fit.B, n.ahead= 110)
U.B= pred.B$pred+ 1.96*pred.B$se
L.B= pred.B$pred-1.96*pred.B$se
ts.plot(ts_data, xlim=c(950,length(ts_data)+110))
lines(U.B, col="blue", lty=2)
lines(L.B, col="blue", lty=2)
lines(1068:1177, pred.B$pred, col="red")

# Adding original data to predictions
ts.plot(ts_data_full, xlim= c(950,length(ts_data)+110), col="black")
lines(U.B, col="blue", lty="dashed")
lines(L.B, col="blue", lty="dashed")
lines((length(ts_data)+1):(length(ts_data)+110), pred.B$pred, col="red")
lines((length(ts_data)+1):(length(ts_data)+110), pred.B$pred, col="red")
```
